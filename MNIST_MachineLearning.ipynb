{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "The MNIST problem its a classic problem for getting started with neural networks.It is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n",
    "\n",
    "There are a lot of parameters that you can fiddle around to see if you can improve the accuracy of the model, some of the parameters are: 1)hidden layer size(width); 2)number of hidden layers; 3)activation function of the hidden layers(relu,sigmoid,softmax etc); 4)batch size; 5)learning rate;\n",
    "\n",
    "After fiddling with the parameters a bit, i was able to achieve 98.5% of accuracy, but it takes 3h40min to train the whole model, so i will leave the original one, where its roughly 96% of accuracy but it takes almost no time to train the model.If you want to test it for yourself, here's what i changed: \n",
    "\n",
    "1)BATCH_SIZE, changed it from 100 to 150;\n",
    "\n",
    "2)hidden_layer_size, changed it from 50 to 5000;\n",
    "\n",
    "3)number of hidden layers, increased from 2 to 10, all of them using the 'relu' activation function;\n",
    "\n",
    "4)NUM_EPOCHS, also increased from 5 to 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick notes\n",
    "\n",
    "This is a project made by a student of data science, so i made all the comments to help myself remember what i am studying and some explanation on things i should keep in mind while making deep learning models.So keep in mind that a lot of the comments its not useful if you are already an experienced data scientist, but to newcomers(such as myself) can be really useful.\n",
    "\n",
    "Thank you for your attention and have fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lembretes\n",
    "\n",
    "Esse é um projeto desenvolvido por um estudante da ciencia de dados, e como é possível ver, o código é inteiramente desenvolvido em inglês(inclusive os comentários) pois é a maneira pela qual eu estou aprendendo sobre Deep Learning, sendo mais fácil escrever tudo em inglês.\n",
    "Se por acaso existirem quaisquer dúvidas sobre o conteúdo, fique á vontade para perguntar diretamente para mim diretamente no github, ou até mesmo por email (luccavick31@hotmail.com). Obrigado pela atenção e se divirta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#TensorFLow includes a data provider for MNIST that we'll use.\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#These datasets will be stored in C:\\Users\\*USERNAME*\\tensorflow_datasets\\...\n",
    "#The first time you download a dataset, it is stored in the respective folder \n",
    "#Every other time, it is automatically loading the copy on your computer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "That's where we load and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfds.load loads a dataset (or downloads and then loads if that's the first time you use it) \n",
    "#The name of the dataset is the only mandatory argument.\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "#With_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "#We will later store that information in the variable mnist_info.\n",
    "#as_supervised=True will load the dataset in a 2-tuple structure (input, target), alternatively, as_supervised=False,\n",
    "#would return a dictionary.\n",
    "\n",
    "#Its preferable to have our inputs and targets separated.\n",
    "\n",
    "#Load the dataset and extract the training and testing dataset with the built references.\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "#By default, TF has training and testing datasets, but no validation sets, and we must split it on our own.\n",
    "\n",
    "#Start by defining the number of validation samples as a % of the train samples.\n",
    "#This is also where we make use of mnist_info (we don't have to count the observations).\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "#Let's cast this number to an integer, as a float may cause an error along the way.\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "#Store the number of test samples in a dedicated variable (instead of using the mnist_info one)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "#Im choosing to use an integer rather than the default float.\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "#Normally, we would like to scale our data in some way to make the result more numerically stable, but in this case\n",
    "#we will simply prefer to have inputs between 0 and 1.\n",
    "#Define a function scale that takes the MNIST image and its label.\n",
    "def scale(image, label):\n",
    "    # Make sure the value is a float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    #Since the possible values for the inputs are 0 to 255 (256 different shades of grey).\n",
    "    #Ff we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1.\n",
    "    image /= 255.\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "#The method .map() allows us to apply a custom transformation to a given dataset.\n",
    "#Get the validation data from mnist_train.\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "#Scale and batch the test data.\n",
    "#It has to have the same magnitude as the train and validation.\n",
    "#There is no need to shuffle it, because we won't be training on the test data.\n",
    "#There would be a single batch, equal to the size of the test data.\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "\n",
    "#Shuffle the data(except the test data).\n",
    "\n",
    "#A buffer_size is needed when you are dealing with enormous datasets, because it wont fit it all in the PC memory when we \n",
    "#shuffle the dataset.\n",
    "#if BUFFER_SIZE=1 => no shuffling will actually happen.\n",
    "#if BUFFER_SIZE >= num samples => shuffling is uniform.\n",
    "#BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling.\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "#There is a shuffle method readily available and we just need to specify the buffer size.\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "#Once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation\n",
    "#Our validation data is equal to 10% of the training set.\n",
    "#We use the .take() method to take that many samples\n",
    "#Finally, we create a batch with a batch size equal to the total number of validation samples\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "#Similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "#Set the batch size.\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#Batch the train data.\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "#Batch the test data.\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "\n",
    "#Takes next batch (it is the only batch)\n",
    "#Because as_supervized=True, we've got a 2-tuple structure\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model\n",
    "When thinking about a deep learning algorithm, we mostly imagine building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "#Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "#You can mess about with the hidden layer size, if you increase it it will take more time to train in a epoch, but the model \n",
    "#will have more accuracy, and there is a point where the accuracy will not increase a significant amount and will take a big\n",
    "#amount of time to train an epoch.\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "#Define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    #The first layer(input layer)\n",
    "    #Each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    #Since this is not a CNN, i don't know how to feed such input into our net, so i must flatten the images\n",
    "    #There is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n",
    "    #or (28x28x1,) = (784,) vector\n",
    "    #This allows us to actually create a feed forward neural network\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    #We want to create only 2 hidden layers.\n",
    "    #tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    #it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    #You can create more hidden layers if you just copy the line above and paste it.\n",
    "    \n",
    "    #the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The compile() accepts the optimizer, the loss function and the metrics as parameters.\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "That's where we train the model we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 8s - loss: 0.4171 - accuracy: 0.8835 - val_loss: 0.2250 - val_accuracy: 0.9360\n",
      "Epoch 2/5\n",
      "540/540 - 7s - loss: 0.1944 - accuracy: 0.9430 - val_loss: 0.1608 - val_accuracy: 0.9525\n",
      "Epoch 3/5\n",
      "540/540 - 7s - loss: 0.1453 - accuracy: 0.9572 - val_loss: 0.1262 - val_accuracy: 0.9637\n",
      "Epoch 4/5\n",
      "540/540 - 7s - loss: 0.1177 - accuracy: 0.9650 - val_loss: 0.1115 - val_accuracy: 0.9640\n",
      "Epoch 5/5\n",
      "540/540 - 7s - loss: 0.0988 - accuracy: 0.9708 - val_loss: 0.1032 - val_accuracy: 0.9697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24d58b5a9c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Determine the maximum number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "#Fit the model, specifying the, training data, the total number of epochs, and the validation data we just created \n",
    "#in the format: (inputs,targets)\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "After training on the training data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has NEVER seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. \n",
    "\n",
    "The test is the absolute final instance. You should not test before you are completely done with adjusting your model.\n",
    "\n",
    "If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1223 - accuracy: 0.9619\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.12. Test accuracy: 96.19%\n"
     ]
    }
   ],
   "source": [
    "#Prints the loss and accuracy of the test dataset.\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the initial model and hyperparameters given in this notebook, the final test accuracy should be roughly around 96%.\n",
    "\n",
    "Each time the code is rerun, we get a different accuracy as the batches are shuffled, the weights are initialized in a different way, etc.\n",
    "\n",
    "This is still a suboptimal solution, so it still have room to grow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
